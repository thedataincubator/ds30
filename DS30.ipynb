{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science in 30 Minutes\n",
    "\n",
    "We do things in Python:\n",
    "\n",
    "1. Python strikes a good balance between flexibility (it's interpreted) and speed (core machine-learning and linear algebra is in c via [cython](http://cython.org/)).\n",
    "1. Python strikes a good balance between being a programming language (it has a lot of language support built up by the community) and specialized machine-learning routines (e.g. [numpy](http://www.numpy.org/), [scipy](http://www.scipy.org/), [scikit learn](http://scikit-learn.org/), [pandas](http://pandas.pydata.org/)).\n",
    "1. The downside of using Python is that installation and dependency management is a pain.  Installing Python is dev-ops work: there's a steep learning curve and you only really need to do it once: unfortunately, you need to be able to do it before you can start using the tools.  If you have the time, you should invest some time in learning about [pip](http://pip.readthedocs.org/en/stable/installing/) and [Virtual Environments](https://virtualenv.pypa.io/en/latest/), although even then, installing scientific python can be a pain.  One of the advantages of being a fellow at [The Data Incubator](https://www.thedataincubator.com/&ref=ds30) is that they get an environement with all the tooling installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing graphics related libraries\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns  # plots are prettier with Seaborn\n",
    "import onlineldavb\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Image\n",
    "from IPython import display\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "# importing useful libraries\n",
    "import simplejson\n",
    "import sys\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "from collections import Counter\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from itertools import islice, chain\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup twitter authorization\n",
    "\n",
    "1. Need to authenticate (can you guess why?).  Creat a twitter account and get your API credentials on the [Twitter API page](http://apps.twitter.com/).\n",
    "1. Secrets not checked into source control (good practice)!\n",
    "1. Using python `stream` (like an infinite list!)\n",
    "1. Handle stream using python `generator`\n",
    "1. *Protip*: `simplejson` is faster than builtin `json`.\n",
    "1. *Protip*: `requests` is easier to use than builtin `urllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"twitter_secrets.json.nogit\") as fh:\n",
    "    secrets = simplejson.loads(fh.read())\n",
    "\n",
    "auth = OAuth1(\n",
    "    secrets[\"api_key\"],\n",
    "    secrets[\"api_secret\"],\n",
    "    secrets[\"access_token\"],\n",
    "    secrets[\"access_token_secret\"]\n",
    ")\n",
    "\n",
    "US_BOUNDING_BOX = \"-125.00,24.94,-66.93,49.59\"\n",
    "def tweet_generator():\n",
    "    stream = requests.post('https://stream.twitter.com/1.1/statuses/filter.json',\n",
    "                         auth=auth,\n",
    "                         stream=True,\n",
    "                         data={\"locations\" : US_BOUNDING_BOX})\n",
    "    \n",
    "    for line in stream.iter_lines():\n",
    "        if not line:  # filter out keep-alive new lines\n",
    "            continue\n",
    "        tweet = simplejson.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            yield tweet['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out twitter stream\n",
    "\n",
    "1. When handling streams, need to use `itertools` (`slice` -> `islice`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in islice(tweet_generator(), 100):\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Statistics\n",
    "\n",
    "1. Raw tweets less interesting than word counts\n",
    "1. Stop words necessary for NLP\n",
    "1. Leverage standard library functions and classes (e.g. `Counter`)\n",
    "1. Don't count iterations manually, use `enumerate`\n",
    "1. Don't print each iteration\n",
    "1. Difference between `\\r` vs. `\\n` and `print` vs. `sys.stdout.write`\n",
    "1. Capture `KeyboardInterrupt` makes output prettier\n",
    "1. `nlargest` should be computed using Heap Queue\n",
    "    ![Heap Queue](heapqueue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DISPLAY_EVERY = 20\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "def nlargest(n, word_scores):\n",
    "    return heapq.nlargest(n, word_scores, key=lambda x: x[1])\n",
    "\n",
    "try:\n",
    "    for k, tweet in enumerate(islice(tweet_generator(), 1000)):\n",
    "        for word in tweet.lower().split():\n",
    "            if word not in stop:\n",
    "                counter[word] += 1\n",
    "        if k % DISPLAY_EVERY == (DISPLAY_EVERY - 1):\n",
    "            sys.stdout.write(\"\\r\" + str(nlargest(10, counter.items())))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    df = pd.DataFrame(nlargest(10, counter.items()), columns=['words', 'count'])\n",
    "    df.set_index('words').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "1. Lists of word counts not as useful\n",
    "1. Wordcloud: size of word ~ frequency (there's a library for that)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "try:\n",
    "    for k, tweet in enumerate(islice(tweet_generator(), 1000)):\n",
    "        for word in tweet.lower().split():\n",
    "            if word not in stop and 'http' not in word:\n",
    "                counter[word] += 1\n",
    "        if k % DISPLAY_EVERY == (DISPLAY_EVERY - 1):\n",
    "            wordcloud = WordCloud().fit_words(counter.items())\n",
    "            plt.axis(\"off\")\n",
    "            display.clear_output(wait=True)\n",
    "            plt.imshow(wordcloud)\n",
    "            display.display(plt.gcf())\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "1. Batch tweets (another generator)\n",
    "1. Online vs offline\n",
    "    1. streaming vs. big data\n",
    "1. Vectorization: convert word to index with a fixed vocabulary\n",
    "    1. Static vs. dynamic vocabulary\n",
    "    ![Vectorization](vectorization.png)\n",
    "1. Use sparse matrices when data is sparse\n",
    "1. We'll use KMeans to illustrate topic modelling.\n",
    "    1. Unsupervised learning.\n",
    "    1. Iteratively assign centers and update them:\n",
    "    ![KMeans](kmeans.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "CLUSTER_SIZE = 4\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=CLUSTER_SIZE)\n",
    "\n",
    "def batch(iterable, size):\n",
    "    \"\"\" batch(\"ABCDEFG\", 3) -> ABC DEF G \"\"\"\n",
    "    sourceiter = iter(iterable)\n",
    "    while True:\n",
    "        batchiter = islice(sourceiter, size)\n",
    "        yield chain([batchiter.next()], batchiter)\n",
    "\n",
    "with open(\"dictnostops.txt\") as fh:\n",
    "    words = [line.strip() for line in fh.readlines()]\n",
    "    word_to_index = { word: k for k, word in enumerate(words) }\n",
    "\n",
    "def wordclouds(wordcounts):\n",
    "    wordclouds = [WordCloud().fit_words(counts) for counts in wordcounts]\n",
    "    fig, axes = plt.subplots(2,2)\n",
    "    display.clear_output(wait=True)\n",
    "    for k, (ax, wordcloud) in enumerate(zip(axes.flatten(), wordclouds)):\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.set_title(\"Topic %d\" % k)\n",
    "    display.display(fig)\n",
    "    fig.clear()\n",
    "\n",
    "try:\n",
    "    for tweets in batch(islice(tweet_generator(), 1000), BATCH_SIZE):\n",
    "        mat = sp.sparse.dok_matrix((BATCH_SIZE, len(words)))\n",
    "        for row, tweet in enumerate(tweets):\n",
    "            for word in tweet.lower().split():\n",
    "                if word in word_to_index:\n",
    "                    mat[row, word_to_index[word]] = 1.\n",
    "        kmeans.partial_fit(mat.tocsr())\n",
    "        wordcounts = [\n",
    "            nlargest(50, zip(words, kmeans.cluster_centers_[i]))\n",
    "            for i in xrange(kmeans.n_clusters)\n",
    "        ]\n",
    "        wordclouds(wordcounts)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "1. [Latent Dirichlet Alloction](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) was develoepd by [David Blei, Andrew Ng, and Michael Jordan](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf).  Here's a [Bayesian](https://en.wikipedia.org/wiki/Bayesian_probability) [Graphical Model](https://en.wikipedia.org/wiki/Graphical_model):\n",
    "    ![LDA](lda1.png)\n",
    "1. LDA is a topic model:\n",
    "    ![LDA](lda2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "D = 1e9\n",
    "BATCH_SIZE = 20\n",
    "olda = onlineldavb.OnlineLDA(words, K, D, 1./K, 1./K, 1024., 0.7)\n",
    "\n",
    "try:\n",
    "    for tweets in batch(islice(tweet_generator(), 1000), BATCH_SIZE):\n",
    "        olda.update_lambda(list(tweets))\n",
    "        wordclouds(olda.topic_words(50))\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want more ....\n",
    "\n",
    "If you enjoyed this and want to learn more about doing data science in industry, consider [applying](https://www.thedataincubator.com/#apply&ref=ds30) to be a fellow at [The Data Incubator](https://www.thedataincubator.com/&ref=ds30)\n",
    "\n",
    "If you would like to hire data scientists, introduce data science corporate training, or partner to bring The Data Incubator to your country, reach out [here](https://www.thedataincubator.com/#hire&ref=ds30).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
